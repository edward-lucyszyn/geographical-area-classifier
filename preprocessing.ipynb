{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "change_type_map = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4, 'Mega Projects': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csvs\n",
    "train_df = gpd.read_file('data/train.geojson', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change_type\n",
      "Residential      148435\n",
      "Commercial       100422\n",
      "Demolition        31509\n",
      "Road              14305\n",
      "Industrial         1324\n",
      "Mega Projects       151\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count number of each change type\n",
    "change_type_counts = train_df['change_type'].value_counts()\n",
    "print(change_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urban_type                 0\n",
      "geography_type             0\n",
      "change_type                0\n",
      "img_red_mean_date1      1954\n",
      "img_green_mean_date1    1954\n",
      "img_blue_mean_date1     1954\n",
      "img_red_std_date1       1954\n",
      "img_green_std_date1     1954\n",
      "img_blue_std_date1      1954\n",
      "img_red_mean_date2      1954\n",
      "img_green_mean_date2    1954\n",
      "img_blue_mean_date2     1954\n",
      "img_red_std_date2       1954\n",
      "img_green_std_date2     1954\n",
      "img_blue_std_date2      1954\n",
      "img_red_mean_date3      1954\n",
      "img_green_mean_date3    1954\n",
      "img_blue_mean_date3     1954\n",
      "img_red_std_date3       1954\n",
      "img_green_std_date3     1954\n",
      "img_blue_std_date3      1954\n",
      "img_red_mean_date4      1954\n",
      "img_green_mean_date4    1954\n",
      "img_blue_mean_date4     1954\n",
      "img_red_std_date4       1954\n",
      "img_green_std_date4     1954\n",
      "img_blue_std_date4      1954\n",
      "img_red_mean_date5      1954\n",
      "img_green_mean_date5    1954\n",
      "img_blue_mean_date5     1954\n",
      "img_red_std_date5       1954\n",
      "img_green_std_date5     1954\n",
      "img_blue_std_date5      1954\n",
      "date0                   1383\n",
      "change_status_date0     1383\n",
      "date1                   1383\n",
      "change_status_date1     1383\n",
      "date2                   1383\n",
      "change_status_date2     1383\n",
      "date3                   1448\n",
      "change_status_date3     1448\n",
      "date4                   1448\n",
      "change_status_date4     1448\n",
      "index                      0\n",
      "geometry                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count number of missing values in each column\n",
    "missing_values = train_df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               urban_type  geography_type  change_type  img_red_mean_date1   \n",
      "change_type                                                                  \n",
      "Commercial              0               0            0                 509  \\\n",
      "Demolition              0               0            0                 242   \n",
      "Industrial              0               0            0                   1   \n",
      "Mega Projects           0               0            0                   0   \n",
      "Residential             0               0            0                1163   \n",
      "Road                    0               0            0                  39   \n",
      "\n",
      "               img_green_mean_date1  img_blue_mean_date1  img_red_std_date1   \n",
      "change_type                                                                   \n",
      "Commercial                      509                  509                509  \\\n",
      "Demolition                      242                  242                242   \n",
      "Industrial                        1                    1                  1   \n",
      "Mega Projects                     0                    0                  0   \n",
      "Residential                    1163                 1163               1163   \n",
      "Road                             39                   39                 39   \n",
      "\n",
      "               img_green_std_date1  img_blue_std_date1  img_red_mean_date2   \n",
      "change_type                                                                  \n",
      "Commercial                     509                 509                 509  \\\n",
      "Demolition                     242                 242                 242   \n",
      "Industrial                       1                   1                   1   \n",
      "Mega Projects                    0                   0                   0   \n",
      "Residential                   1163                1163                1163   \n",
      "Road                            39                  39                  39   \n",
      "\n",
      "               ...  date1  change_status_date1  date2  change_status_date2   \n",
      "change_type    ...                                                           \n",
      "Commercial     ...    364                  364    364                  364  \\\n",
      "Demolition     ...    112                  112    112                  112   \n",
      "Industrial     ...     13                   13     13                   13   \n",
      "Mega Projects  ...      2                    2      2                    2   \n",
      "Residential    ...    817                  817    817                  817   \n",
      "Road           ...     75                   75     75                   75   \n",
      "\n",
      "               date3  change_status_date3  date4  change_status_date4  index   \n",
      "change_type                                                                    \n",
      "Commercial       416                  416    416                  416      0  \\\n",
      "Demolition       113                  113    113                  113      0   \n",
      "Industrial        13                   13     13                   13      0   \n",
      "Mega Projects      2                    2      2                    2      0   \n",
      "Residential      828                  828    828                  828      0   \n",
      "Road              76                   76     76                   76      0   \n",
      "\n",
      "               geometry  \n",
      "change_type              \n",
      "Commercial            0  \n",
      "Demolition            0  \n",
      "Industrial            0  \n",
      "Mega Projects         0  \n",
      "Residential           0  \n",
      "Road                  0  \n",
      "\n",
      "[6 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "# Count number of missing values in each column for each change type\n",
    "none_counts = train_df.groupby('change_type').apply(lambda x: x.isnull().sum())\n",
    "print(none_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               urban_type  geography_type  change_type  img_red_mean_date1   \n",
      "change_type                                                                  \n",
      "Commercial            0.0             0.0          0.0            0.005069  \\\n",
      "Demolition            0.0             0.0          0.0            0.007680   \n",
      "Industrial            0.0             0.0          0.0            0.000755   \n",
      "Mega Projects         0.0             0.0          0.0            0.000000   \n",
      "Residential           0.0             0.0          0.0            0.007835   \n",
      "Road                  0.0             0.0          0.0            0.002726   \n",
      "\n",
      "               img_green_mean_date1  img_blue_mean_date1  img_red_std_date1   \n",
      "change_type                                                                   \n",
      "Commercial                 0.005069             0.005069           0.005069  \\\n",
      "Demolition                 0.007680             0.007680           0.007680   \n",
      "Industrial                 0.000755             0.000755           0.000755   \n",
      "Mega Projects              0.000000             0.000000           0.000000   \n",
      "Residential                0.007835             0.007835           0.007835   \n",
      "Road                       0.002726             0.002726           0.002726   \n",
      "\n",
      "               img_green_std_date1  img_blue_std_date1  img_red_mean_date2   \n",
      "change_type                                                                  \n",
      "Commercial                0.005069            0.005069            0.005069  \\\n",
      "Demolition                0.007680            0.007680            0.007680   \n",
      "Industrial                0.000755            0.000755            0.000755   \n",
      "Mega Projects             0.000000            0.000000            0.000000   \n",
      "Residential               0.007835            0.007835            0.007835   \n",
      "Road                      0.002726            0.002726            0.002726   \n",
      "\n",
      "               ...     date1  change_status_date1     date2   \n",
      "change_type    ...                                            \n",
      "Commercial     ...  0.003625             0.003625  0.003625  \\\n",
      "Demolition     ...  0.003555             0.003555  0.003555   \n",
      "Industrial     ...  0.009819             0.009819  0.009819   \n",
      "Mega Projects  ...  0.013245             0.013245  0.013245   \n",
      "Residential    ...  0.005504             0.005504  0.005504   \n",
      "Road           ...  0.005243             0.005243  0.005243   \n",
      "\n",
      "               change_status_date2     date3  change_status_date3     date4   \n",
      "change_type                                                                   \n",
      "Commercial                0.003625  0.004143             0.004143  0.004143  \\\n",
      "Demolition                0.003555  0.003586             0.003586  0.003586   \n",
      "Industrial                0.009819  0.009819             0.009819  0.009819   \n",
      "Mega Projects             0.013245  0.013245             0.013245  0.013245   \n",
      "Residential               0.005504  0.005578             0.005578  0.005578   \n",
      "Road                      0.005243  0.005313             0.005313  0.005313   \n",
      "\n",
      "               change_status_date4  index  geometry  \n",
      "change_type                                          \n",
      "Commercial                0.004143    0.0       0.0  \n",
      "Demolition                0.003586    0.0       0.0  \n",
      "Industrial                0.009819    0.0       0.0  \n",
      "Mega Projects             0.013245    0.0       0.0  \n",
      "Residential               0.005578    0.0       0.0  \n",
      "Road                      0.005313    0.0       0.0  \n",
      "\n",
      "[6 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "# Count number of missing values in each column for each change type as a percentage\n",
    "none_counts = train_df.groupby('change_type').apply(lambda x: x.isnull().sum() / len(x))\n",
    "print(none_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio is very light, it is possible to delete the lines with missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates and Colors Filtering\n",
    "\n",
    "### Ordering and computing time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date0       date1       date2       date3       date4\n",
      "0       01-08-2018  09-12-2013  10-09-2016  22-07-2019  24-07-2017\n",
      "1       01-08-2018  09-12-2013  10-09-2016  22-07-2019  24-07-2017\n",
      "2       01-08-2018  09-12-2013  10-09-2016  22-07-2019  24-07-2017\n",
      "3       01-08-2018  09-12-2013  10-09-2016  22-07-2019  24-07-2017\n",
      "4       01-08-2018  09-12-2013  10-09-2016  22-07-2019  24-07-2017\n",
      "...            ...         ...         ...         ...         ...\n",
      "296141  19-11-2014  25-02-2017  27-01-2014  28-03-2018  28-12-2015\n",
      "296142  19-11-2014  25-02-2017  27-01-2014  28-03-2018  28-12-2015\n",
      "296143  19-11-2014  25-02-2017  27-01-2014  28-03-2018  28-12-2015\n",
      "296144  19-11-2014  25-02-2017  27-01-2014  28-03-2018  28-12-2015\n",
      "296145  19-11-2014  25-02-2017  27-01-2014  28-03-2018  28-12-2015\n",
      "\n",
      "[296146 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create the table of train data\n",
    "train_dates = train_df[[f'date{i}' for i in range(0, 5)]]\n",
    "print(train_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Calculating arsort indexes of dates\n",
    "def index_sort_by_dates(date_row):\n",
    "    return np.argsort(pd.to_datetime(date_row, format='%d-%m-%Y'))\n",
    "\n",
    "# Test index_sort_by_dates\n",
    "print(index_sort_by_dates(['18-09-2010', '18-09-2009', '18-09-2000']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date0  date1  date2  date3  date4\n",
      "0           1      2      4      0      3\n",
      "1           1      2      4      0      3\n",
      "2           1      2      4      0      3\n",
      "3           1      2      4      0      3\n",
      "4           1      2      4      0      3\n",
      "...       ...    ...    ...    ...    ...\n",
      "296141      2      0      4      1      3\n",
      "296142      2      0      4      1      3\n",
      "296143      2      0      4      1      3\n",
      "296144      2      0      4      1      3\n",
      "296145      2      0      4      1      3\n",
      "\n",
      "[296146 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create the table of arsort indexes of dates\n",
    "train_index = train_dates[[f'date{i}' for i in range(0, 5)]].apply(index_sort_by_dates, axis=1)\n",
    "print(train_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Times Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[365, 365, 366, 2922]\n"
     ]
    }
   ],
   "source": [
    "def time_interval(row):\n",
    "    date_row = row[:5]\n",
    "    index_row = row[5:]\n",
    "    return [int((pd.to_datetime(date_row[index_row[i + 1]], format='%d-%m-%Y') - pd.to_datetime(date_row[index_row[i]], format='%d-%m-%Y')).days) if date_row[index_row[i]] is not None and date_row[index_row[i + 1]] is not None else None for i in range(4)]\n",
    "\n",
    "# Test time_interval\n",
    "print(time_interval(['18-09-2020', '18-09-2009', '18-09-2010', '18-09-2011', '18-09-2012', 1, 2, 3, 4, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        time_interval1  time_interval2  time_interval3  time_interval4\n",
      "0               1006.0           317.0           373.0           355.0\n",
      "1               1006.0           317.0           373.0           355.0\n",
      "2               1006.0           317.0           373.0           355.0\n",
      "3               1006.0           317.0           373.0           355.0\n",
      "4               1006.0           317.0           373.0           355.0\n",
      "...                ...             ...             ...             ...\n",
      "296141           296.0           404.0           425.0           396.0\n",
      "296142           296.0           404.0           425.0           396.0\n",
      "296143           296.0           404.0           425.0           396.0\n",
      "296144           296.0           404.0           425.0           396.0\n",
      "296145           296.0           404.0           425.0           396.0\n",
      "\n",
      "[296146 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create the table of time intervals\n",
    "train_intervals = pd.concat([train_dates, train_index], axis=1).apply(time_interval, axis=1)\n",
    "train_intervals = pd.DataFrame(train_intervals.tolist(), columns=['time_interval1', 'time_interval2', 'time_interval3', 'time_interval4'])\n",
    "print(train_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_interval1    1383\n",
      "time_interval2    1383\n",
      "time_interval3    1448\n",
      "time_interval4    1448\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count number of None\n",
    "none_counts = train_intervals.isnull().sum()\n",
    "print(none_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_interval1    0\n",
      "time_interval2    0\n",
      "time_interval3    0\n",
      "time_interval4    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replace None with mean\n",
    "train_intervals = train_intervals.fillna(train_intervals.mean())\n",
    "none_counts = train_intervals.isnull().sum()\n",
    "print(none_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296146, 4)\n",
      "[[ 3.05075593 -0.99085077 -0.58895366 -0.61472004]\n",
      " [ 3.05075593 -0.99085077 -0.58895366 -0.61472004]\n",
      " [ 3.05075593 -0.99085077 -0.58895366 -0.61472004]\n",
      " ...\n",
      " [-0.86639699 -0.42730469 -0.28552383 -0.36099474]\n",
      " [-0.86639699 -0.42730469 -0.28552383 -0.36099474]\n",
      " [-0.86639699 -0.42730469 -0.28552383 -0.36099474]]\n"
     ]
    }
   ],
   "source": [
    "# Creating array\n",
    "train_intervals = np.asarray(train_intervals).reshape(-1, 4)\n",
    "print(train_intervals.shape)\n",
    "\n",
    "# Normalizing the data\n",
    "if train_intervals.std(axis=0).any():\n",
    "    train_intervals = (train_intervals - train_intervals.mean(axis=0)) / train_intervals.std(axis=0)\n",
    "else:\n",
    "    train_intervals = (train_intervals - train_intervals.mean(axis=0))\n",
    "\n",
    "print(train_intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 4, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "# Define the get_ordered_values function\n",
    "def get_ordered_values(row):\n",
    "    data = row[:5]\n",
    "    indexes = row[5:]\n",
    "    return [data[int(indexes[i])] for i in range(5)]\n",
    "\n",
    "# Test get_ordered_values\n",
    "print(get_ordered_values([1, 2, 3, 4, 5, 4, 3, 2, 1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        img_red_mean_date1  img_red_mean_date2  img_red_mean_date3   \n",
      "0               125.773062          150.766726          104.614233  \\\n",
      "1               133.097679          184.480155          110.445556   \n",
      "2               120.713490          148.150431           98.734466   \n",
      "3               114.819776          148.322747           90.506679   \n",
      "4               141.514462          170.365008          155.293576   \n",
      "...                    ...                 ...                 ...   \n",
      "296141          211.421955          239.297084          186.380103   \n",
      "296142          196.476812          162.912319          170.780797   \n",
      "296143           68.845906          111.304320          108.650548   \n",
      "296144           98.718266          137.374613          157.947368   \n",
      "296145          104.533730          176.841270          176.222222   \n",
      "\n",
      "        img_red_mean_date4  img_red_mean_date5  \n",
      "0                93.371775           92.291347  \n",
      "1                96.071674          111.001421  \n",
      "2               101.212148           94.519603  \n",
      "3                94.463311           91.731693  \n",
      "4               151.883646          108.124000  \n",
      "...                    ...                 ...  \n",
      "296141          140.346141          163.659691  \n",
      "296142          103.760507          150.684783  \n",
      "296143           68.845906           61.174726  \n",
      "296144           98.718266          164.798762  \n",
      "296145          104.533730          168.692460  \n",
      "\n",
      "[296146 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Red Mean\n",
    "train_red_mean = pd.concat([train_df[[f'img_red_mean_date{i}' for i in range(1, 6)]], train_index], axis=1).apply(get_ordered_values, axis=1)\n",
    "train_red_mean = pd.DataFrame(train_red_mean.tolist(), columns=[f'img_red_mean_date{i}' for i in range(1, 6)])\n",
    "print(train_red_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with mean\n",
    "train_red_mean = train_red_mean.fillna(train_red_mean.mean())\n",
    "none_counts = train_red_mean.isnull().sum()\n",
    "print(none_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        img_red_mean_date1  img_red_mean_date2  img_red_mean_date3   \n",
      "0                 0.210286            0.797181           -0.432982  \\\n",
      "1                 0.398533            1.646722           -0.287877   \n",
      "2                 0.080252            0.731253           -0.579292   \n",
      "3                -0.071220            0.735595           -0.784030   \n",
      "4                 0.614850            1.291036            0.828107   \n",
      "...                    ...                 ...                 ...   \n",
      "296141            2.411516            3.028049            1.601654   \n",
      "296142            2.027417            1.103237            1.213486   \n",
      "296143           -1.252778           -0.197228           -0.332543   \n",
      "296144           -0.485039            0.459714            0.894143   \n",
      "296145           -0.335578            1.454231            1.348888   \n",
      "\n",
      "        img_red_mean_date4  img_red_mean_date5  \n",
      "0                -0.646043           -0.680883  \n",
      "1                -0.580825           -0.208969  \n",
      "2                -0.456653           -0.624681  \n",
      "3                -0.619676           -0.694999  \n",
      "4                 0.767360           -0.281545  \n",
      "...                    ...                 ...  \n",
      "296141            0.488662            1.119202  \n",
      "296142           -0.395095            0.791943  \n",
      "296143           -1.238486           -1.465721  \n",
      "296144           -0.516894            1.147932  \n",
      "296145           -0.376417            1.246141  \n",
      "\n",
      "[296146 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Normalize data\n",
    "train_red_mean = (train_red_mean - train_red_mean.mean()) / train_red_mean.std()\n",
    "print(train_red_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red-Green-Blue Means and Svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        img_green_std_date1  img_green_std_date2  img_green_std_date3   \n",
      "0                  0.122145             1.546239             0.422036  \\\n",
      "1                 -0.341687             0.877316             0.435797   \n",
      "2                 -0.083731             1.583832             0.456068   \n",
      "3                  0.100980             1.324257             0.390201   \n",
      "4                  0.545223             1.068349             0.769790   \n",
      "...                     ...                  ...                  ...   \n",
      "296141            -0.332977             0.412993             0.292753   \n",
      "296142             1.268435             1.305857             1.978747   \n",
      "296143            -0.929067            -0.565057            -0.039399   \n",
      "296144             0.004296             0.306057             1.968091   \n",
      "296145             0.001648             0.948938             1.094236   \n",
      "\n",
      "        img_green_std_date4  img_green_std_date5  img_red_std_date1   \n",
      "0                  0.150581             0.394846           0.014283  \\\n",
      "1                 -0.273848             0.198926          -0.299173   \n",
      "2                 -0.302019             0.559006           0.060863   \n",
      "3                 -0.169199             0.387630           0.412218   \n",
      "4                  2.332150             0.203771          -0.247719   \n",
      "...                     ...                  ...                ...   \n",
      "296141            -0.336515             0.184055          -0.404067   \n",
      "296142            -0.198012             2.204454           0.559317   \n",
      "296143            -0.904845            -0.983235          -0.793350   \n",
      "296144             0.028309            -0.179623           0.113337   \n",
      "296145             0.025661            -0.505645           0.123200   \n",
      "\n",
      "        img_red_std_date2  img_red_std_date3  img_red_std_date4   \n",
      "0                1.964438           0.300915           0.122761  \\\n",
      "1                1.027759           0.093144          -0.203504   \n",
      "2                2.149424           0.342090          -0.251063   \n",
      "3                1.641187           0.133799          -0.072514   \n",
      "4                0.334478           0.751340           1.626278   \n",
      "...                   ...                ...                ...   \n",
      "296141          -0.084143           0.672229          -0.201037   \n",
      "296142           1.990757           1.847416           0.212687   \n",
      "296143          -0.388872          -0.106208          -0.763828   \n",
      "296144           0.354466           1.904911           0.116599   \n",
      "296145           0.941958           1.179925           0.126176   \n",
      "\n",
      "        img_red_std_date5  ...  img_blue_std_date1  img_blue_std_date2   \n",
      "0                0.876367  ...           -0.027387            1.310324  \\\n",
      "1                0.693241  ...           -0.368473            0.757500   \n",
      "2                1.059512  ...           -0.207580            1.396342   \n",
      "3                0.828168  ...           -0.107063            1.152456   \n",
      "4                0.043126  ...            1.167530            1.718865   \n",
      "...                   ...  ...                 ...                 ...   \n",
      "296141           0.326063  ...           -0.300112            0.539705   \n",
      "296142           1.761284  ...            1.760080            1.823522   \n",
      "296143          -0.912537  ...           -1.025584           -0.482120   \n",
      "296144          -0.257089  ...           -0.224703            0.364978   \n",
      "296145          -0.492931  ...           -0.196674            1.080211   \n",
      "\n",
      "        img_blue_std_date3  img_blue_std_date4  img_blue_std_date5   \n",
      "0                 0.653366            0.006945            0.280625  \\\n",
      "1                 0.750142           -0.227139            0.114763   \n",
      "2                 0.706653           -0.284547            0.449742   \n",
      "3                 0.745825           -0.424063            0.291357   \n",
      "4                 0.826178            1.954031            0.216637   \n",
      "...                    ...                 ...                 ...   \n",
      "296141            0.497878           -0.516428           -0.113372   \n",
      "296142            2.286949           -0.300520            2.505978   \n",
      "296143           -0.063211           -1.016204           -0.943717   \n",
      "296144            2.031670           -0.213616           -0.158401   \n",
      "296145            1.518939           -0.185527           -0.520805   \n",
      "\n",
      "        img_blue_mean_date1  img_blue_mean_date2  img_blue_mean_date3   \n",
      "0                  0.729812             1.055422            -0.276088  \\\n",
      "1                  0.794080             1.751800            -0.220660   \n",
      "2                  0.422982             1.033438            -0.377612   \n",
      "3                  0.305662             1.022508            -0.566461   \n",
      "4                  2.109696             2.104016             0.507887   \n",
      "...                     ...                  ...                  ...   \n",
      "296141             1.992467             2.898603             1.379505   \n",
      "296142             1.805213             0.315858             0.171067   \n",
      "296143            -1.721049            -0.875453            -1.095649   \n",
      "296144            -1.097759             0.053016             0.511544   \n",
      "296145            -0.893941             1.262701             1.146468   \n",
      "\n",
      "        img_blue_mean_date4  img_blue_mean_date5  \n",
      "0                 -0.610171            -0.923798  \n",
      "1                 -0.584355            -0.579376  \n",
      "2                 -0.447648            -0.860684  \n",
      "3                 -0.759181            -0.927855  \n",
      "4                  2.775975            -0.501266  \n",
      "...                     ...                  ...  \n",
      "296141            -0.373553             0.959395  \n",
      "296142            -1.161623             0.173933  \n",
      "296143            -1.681334            -1.761338  \n",
      "296144            -1.090094             0.944493  \n",
      "296145            -0.896756             1.284233  \n",
      "\n",
      "[296146 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "config = {'red_mean', 'blue_mean', 'green_mean', 'red_std', 'blue_std', 'green_std'}\n",
    "train_colors = pd.DataFrame()\n",
    "\n",
    "for element in config:\n",
    "    train_element = pd.concat([train_df[[f'img_{element}_date{i}' for i in range(1, 6)]], train_index], axis=1).apply(get_ordered_values, axis=1)\n",
    "    train_element = pd.DataFrame(train_element.tolist(), columns=[f'img_{element}_date{i}' for i in range(1, 6)])\n",
    "    train_element = train_element.fillna(train_element.mean())\n",
    "    train_element = (train_element - train_element.mean()) / train_element.std()\n",
    "    train_colors = pd.concat([train_colors, train_element], axis=1)\n",
    "\n",
    "print(train_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296146, 30)\n"
     ]
    }
   ],
   "source": [
    "# Creating array\n",
    "train_colors = np.asarray(train_colors).reshape(-1, 30)\n",
    "print(train_colors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hot-one Encoding for Urban and Geographical data\n",
    "\n",
    "### Change Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296146,)\n"
     ]
    }
   ],
   "source": [
    "# Get change_status by ascending dates\n",
    "train_change_status = pd.concat([train_df[[f'change_status_date{i}' for i in range(0, 5)]], train_index], axis=1).apply(get_ordered_values, axis=1)\n",
    "print(train_change_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Filtering\n",
    "\n",
    "### Computing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4c/2cw0jjvs6nb5v4xdxk3ty9s40000gn/T/ipykernel_17336/2481818807.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  train_area = np.asarray(train_df[['geometry']].area)\n"
     ]
    }
   ],
   "source": [
    "# Area of the building\n",
    "train_area = np.asarray(train_df[['geometry']].area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_convex_hull_area(geometry):\n",
    "    try:\n",
    "        convex_hull = geometry.convex_hull\n",
    "        return convex_hull.area\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Calculate the area of the convex hull for each geometry\n",
    "train_convex_area = train_df['geometry'].apply(calculate_convex_hull_area)\n",
    "\n",
    "# Convert to numpy array\n",
    "train_convex_area = np.asarray(train_convex_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convex hull ratio\n",
    "train_convex_ratio = train_area / train_convex_area\n",
    "train_convex_ratio = train_convex_ratio.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the area parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non convex areas: 42369\n",
      "Ratio of non convex areas of 14.306794621571791%.\n",
      "Mean area: 5.792798723701533e-07 and std area: 6.544386914765251e-05\n"
     ]
    }
   ],
   "source": [
    "# Number of non convex areas\n",
    "non_convex_areas_n = np.sum(train_convex_ratio < 0.99)\n",
    "non_convex_areas_ratio = non_convex_areas_n / len(train_convex_ratio)\n",
    "print('Number of non convex areas: ' + str(non_convex_areas_n))\n",
    "print('Ratio of non convex areas of ' + str(non_convex_areas_ratio*100) + '%.')\n",
    "mean_area = np.mean(train_area)\n",
    "\n",
    "# Mean and Standard deviation of areas\n",
    "std_area = np.std(train_area)\n",
    "print('Mean area: ' + str(mean_area) + ' and std area: ' + str(std_area))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coherent ! a road takes less place than a residential area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00363946  0.28623304]\n",
      " [-0.00213689  0.28623304]\n",
      " [ 0.00369309  0.28623304]\n",
      " ...\n",
      " [-0.00868493  0.28623304]\n",
      " [-0.00882395  0.28623304]\n",
      " [-0.00881304  0.28623304]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize area\n",
    "train_area = (train_area - train_area.mean()) / train_area.std()\n",
    "train_convex_ratio = (train_convex_ratio - train_convex_ratio.mean()) / train_convex_ratio.std()\n",
    "\n",
    "# Stacking the features\n",
    "train_area_features = np.hstack([train_area.reshape(-1, 1), train_convex_ratio])\n",
    "print(train_area_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate all the tables and PCA\n",
    "\n",
    "### Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296146, 36)\n"
     ]
    }
   ],
   "source": [
    "# Concatenating the features\n",
    "train_features = np.hstack([train_area_features, train_colors, train_intervals])\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Applying PCA\u001b[39;00m\n\u001b[1;32m      2\u001b[0m W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(train_features\u001b[38;5;241m.\u001b[39mT, train_features) \u001b[38;5;66;03m# compute covariance matrix\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m eigval, eigvec \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# compute eigenvalues and eigenvectors of covariance matrix\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Sort eigenvalues and eigenvectors\u001b[39;00m\n\u001b[1;32m      6\u001b[0m idx \u001b[38;5;241m=\u001b[39m eigval\u001b[38;5;241m.\u001b[39margsort()[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# Sort eigenvalues\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36meig\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/linalg/linalg.py:1298\u001b[0m, in \u001b[0;36meig\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   1296\u001b[0m _assert_stacked_2d(a)\n\u001b[1;32m   1297\u001b[0m _assert_stacked_square(a)\n\u001b[0;32m-> 1298\u001b[0m \u001b[43m_assert_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1299\u001b[0m t, result_t \u001b[38;5;241m=\u001b[39m _commonType(a)\n\u001b[1;32m   1301\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(\n\u001b[1;32m   1302\u001b[0m     _raise_linalgerror_eigenvalues_nonconvergence)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/linalg/linalg.py:195\u001b[0m, in \u001b[0;36m_assert_finite\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m isfinite(a)\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m--> 195\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArray must not contain infs or NaNs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "# Applying PCA\n",
    "W = np.dot(train_features.T, train_features) # compute covariance matrix\n",
    "eigval, eigvec = np.linalg.eig(W) # compute eigenvalues and eigenvectors of covariance matrix\n",
    "\n",
    "# Sort eigenvalues and eigenvectors\n",
    "idx = eigval.argsort()[::-1] # Sort eigenvalues\n",
    "eigvec = eigvec[:,idx] # Sort eigenvectors according to eigenvalues\n",
    "eigval = eigval[idx] # Sort eigenvalues\n",
    "\n",
    "print(np.sum(eigval[:1]) / np.sum(eigval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the best k to achieve 85% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=12 ; percent=86%.\n"
     ]
    }
   ],
   "source": [
    "k = 1\n",
    "while k < len(eigval) and (np.sum(eigval[:k])/np.sum(eigval)) < 0.85:   \n",
    "    k += 1\n",
    "\n",
    "print('k=' + str(k), '; percent=' + str(int(np.sum(eigval[:k]) / np.sum(eigval)*100)) + '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(292758, 12)\n",
      "[[-1.34347304 -1.74579613  0.66557092 ... -0.88468115  2.68653112\n",
      "   0.21884086]\n",
      " [-1.48549065 -0.05429729  0.71451344 ... -0.22095987  2.9346425\n",
      "   0.41605621]\n",
      " [-1.19865268 -1.68989846  0.8564808  ... -1.05744206  2.43823516\n",
      "   0.11941088]\n",
      " ...\n",
      " [ 5.13592389 -0.51058606  0.07969247 ... -0.13419465 -0.14683372\n",
      "  -0.10813903]\n",
      " [-1.35910374 -0.82818722 -0.00697445 ...  2.56089568 -0.75172278\n",
      "   0.16893018]\n",
      " [-2.49919984  0.36370635  0.29944503 ...  2.22705814 -0.76826445\n",
      "   0.16163682]]\n"
     ]
    }
   ],
   "source": [
    "# Selecting the first k principal components\n",
    "train_features = np.dot(train_features, eigvec[:,:k])\n",
    "print(train_features.shape)\n",
    "print(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train features\n",
    "np.save('preprocessed_data.npy', train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         1\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "4         0\n",
      "         ..\n",
      "296141    3\n",
      "296142    2\n",
      "296143    2\n",
      "296144    2\n",
      "296145    2\n",
      "Name: change_type, Length: 292758, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Save the train labels\n",
    "train_y = train_df['change_type'].apply(lambda x: change_type_map[x])\n",
    "print(train_y)\n",
    "\n",
    "np.save('preprocessed_labels.npy', train_df['change_type'].map(change_type_map).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt the test data to the PCA model\n",
    "test_df = gpd.read_file('test.geojson', index_col=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
