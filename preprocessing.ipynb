{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "change_type_map = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4, 'Mega Projects': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csvs\n",
    "train_df = gpd.read_file('data/train.geojson', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change_type\n",
      "Residential      148435\n",
      "Commercial       100422\n",
      "Demolition        31509\n",
      "Road              14305\n",
      "Industrial         1324\n",
      "Mega Projects       151\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count number of each change type\n",
    "change_type_counts = train_df['change_type'].value_counts()\n",
    "print(change_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urban_type                 0\n",
      "geography_type             0\n",
      "change_type                0\n",
      "img_red_mean_date1      1954\n",
      "img_green_mean_date1    1954\n",
      "img_blue_mean_date1     1954\n",
      "img_red_std_date1       1954\n",
      "img_green_std_date1     1954\n",
      "img_blue_std_date1      1954\n",
      "img_red_mean_date2      1954\n",
      "img_green_mean_date2    1954\n",
      "img_blue_mean_date2     1954\n",
      "img_red_std_date2       1954\n",
      "img_green_std_date2     1954\n",
      "img_blue_std_date2      1954\n",
      "img_red_mean_date3      1954\n",
      "img_green_mean_date3    1954\n",
      "img_blue_mean_date3     1954\n",
      "img_red_std_date3       1954\n",
      "img_green_std_date3     1954\n",
      "img_blue_std_date3      1954\n",
      "img_red_mean_date4      1954\n",
      "img_green_mean_date4    1954\n",
      "img_blue_mean_date4     1954\n",
      "img_red_std_date4       1954\n",
      "img_green_std_date4     1954\n",
      "img_blue_std_date4      1954\n",
      "img_red_mean_date5      1954\n",
      "img_green_mean_date5    1954\n",
      "img_blue_mean_date5     1954\n",
      "img_red_std_date5       1954\n",
      "img_green_std_date5     1954\n",
      "img_blue_std_date5      1954\n",
      "date0                   1383\n",
      "change_status_date0     1383\n",
      "date1                   1383\n",
      "change_status_date1     1383\n",
      "date2                   1383\n",
      "change_status_date2     1383\n",
      "date3                   1448\n",
      "change_status_date3     1448\n",
      "date4                   1448\n",
      "change_status_date4     1448\n",
      "index                      0\n",
      "geometry                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count number of missing values in each column\n",
    "missing_values = train_df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               urban_type  geography_type  change_type  img_red_mean_date1   \n",
      "change_type                                                                  \n",
      "Commercial              0               0            0                 509  \\\n",
      "Demolition              0               0            0                 242   \n",
      "Industrial              0               0            0                   1   \n",
      "Mega Projects           0               0            0                   0   \n",
      "Residential             0               0            0                1163   \n",
      "Road                    0               0            0                  39   \n",
      "\n",
      "               img_green_mean_date1  img_blue_mean_date1  img_red_std_date1   \n",
      "change_type                                                                   \n",
      "Commercial                      509                  509                509  \\\n",
      "Demolition                      242                  242                242   \n",
      "Industrial                        1                    1                  1   \n",
      "Mega Projects                     0                    0                  0   \n",
      "Residential                    1163                 1163               1163   \n",
      "Road                             39                   39                 39   \n",
      "\n",
      "               img_green_std_date1  img_blue_std_date1  img_red_mean_date2   \n",
      "change_type                                                                  \n",
      "Commercial                     509                 509                 509  \\\n",
      "Demolition                     242                 242                 242   \n",
      "Industrial                       1                   1                   1   \n",
      "Mega Projects                    0                   0                   0   \n",
      "Residential                   1163                1163                1163   \n",
      "Road                            39                  39                  39   \n",
      "\n",
      "               ...  date1  change_status_date1  date2  change_status_date2   \n",
      "change_type    ...                                                           \n",
      "Commercial     ...    364                  364    364                  364  \\\n",
      "Demolition     ...    112                  112    112                  112   \n",
      "Industrial     ...     13                   13     13                   13   \n",
      "Mega Projects  ...      2                    2      2                    2   \n",
      "Residential    ...    817                  817    817                  817   \n",
      "Road           ...     75                   75     75                   75   \n",
      "\n",
      "               date3  change_status_date3  date4  change_status_date4  index   \n",
      "change_type                                                                    \n",
      "Commercial       416                  416    416                  416      0  \\\n",
      "Demolition       113                  113    113                  113      0   \n",
      "Industrial        13                   13     13                   13      0   \n",
      "Mega Projects      2                    2      2                    2      0   \n",
      "Residential      828                  828    828                  828      0   \n",
      "Road              76                   76     76                   76      0   \n",
      "\n",
      "               geometry  \n",
      "change_type              \n",
      "Commercial            0  \n",
      "Demolition            0  \n",
      "Industrial            0  \n",
      "Mega Projects         0  \n",
      "Residential           0  \n",
      "Road                  0  \n",
      "\n",
      "[6 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "# Count number of missing values in each column for each change type\n",
    "none_counts = train_df.groupby('change_type').apply(lambda x: x.isnull().sum())\n",
    "print(none_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               urban_type  geography_type  change_type  img_red_mean_date1   \n",
      "change_type                                                                  \n",
      "Commercial            0.0             0.0          0.0            0.005069  \\\n",
      "Demolition            0.0             0.0          0.0            0.007680   \n",
      "Industrial            0.0             0.0          0.0            0.000755   \n",
      "Mega Projects         0.0             0.0          0.0            0.000000   \n",
      "Residential           0.0             0.0          0.0            0.007835   \n",
      "Road                  0.0             0.0          0.0            0.002726   \n",
      "\n",
      "               img_green_mean_date1  img_blue_mean_date1  img_red_std_date1   \n",
      "change_type                                                                   \n",
      "Commercial                 0.005069             0.005069           0.005069  \\\n",
      "Demolition                 0.007680             0.007680           0.007680   \n",
      "Industrial                 0.000755             0.000755           0.000755   \n",
      "Mega Projects              0.000000             0.000000           0.000000   \n",
      "Residential                0.007835             0.007835           0.007835   \n",
      "Road                       0.002726             0.002726           0.002726   \n",
      "\n",
      "               img_green_std_date1  img_blue_std_date1  img_red_mean_date2   \n",
      "change_type                                                                  \n",
      "Commercial                0.005069            0.005069            0.005069  \\\n",
      "Demolition                0.007680            0.007680            0.007680   \n",
      "Industrial                0.000755            0.000755            0.000755   \n",
      "Mega Projects             0.000000            0.000000            0.000000   \n",
      "Residential               0.007835            0.007835            0.007835   \n",
      "Road                      0.002726            0.002726            0.002726   \n",
      "\n",
      "               ...     date1  change_status_date1     date2   \n",
      "change_type    ...                                            \n",
      "Commercial     ...  0.003625             0.003625  0.003625  \\\n",
      "Demolition     ...  0.003555             0.003555  0.003555   \n",
      "Industrial     ...  0.009819             0.009819  0.009819   \n",
      "Mega Projects  ...  0.013245             0.013245  0.013245   \n",
      "Residential    ...  0.005504             0.005504  0.005504   \n",
      "Road           ...  0.005243             0.005243  0.005243   \n",
      "\n",
      "               change_status_date2     date3  change_status_date3     date4   \n",
      "change_type                                                                   \n",
      "Commercial                0.003625  0.004143             0.004143  0.004143  \\\n",
      "Demolition                0.003555  0.003586             0.003586  0.003586   \n",
      "Industrial                0.009819  0.009819             0.009819  0.009819   \n",
      "Mega Projects             0.013245  0.013245             0.013245  0.013245   \n",
      "Residential               0.005504  0.005578             0.005578  0.005578   \n",
      "Road                      0.005243  0.005313             0.005313  0.005313   \n",
      "\n",
      "               change_status_date4  index  geometry  \n",
      "change_type                                          \n",
      "Commercial                0.004143    0.0       0.0  \n",
      "Demolition                0.003586    0.0       0.0  \n",
      "Industrial                0.009819    0.0       0.0  \n",
      "Mega Projects             0.013245    0.0       0.0  \n",
      "Residential               0.005578    0.0       0.0  \n",
      "Road                      0.005313    0.0       0.0  \n",
      "\n",
      "[6 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "# Count number of missing values in each column for each change type as a percentage\n",
    "none_counts = train_df.groupby('change_type').apply(lambda x: x.isnull().sum() / len(x))\n",
    "print(none_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio is very light, it is possible to fill the lines with missing data with the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urban_type              0\n",
      "geography_type          0\n",
      "change_type             0\n",
      "img_red_mean_date1      0\n",
      "img_green_mean_date1    0\n",
      "img_blue_mean_date1     0\n",
      "img_red_std_date1       0\n",
      "img_green_std_date1     0\n",
      "img_blue_std_date1      0\n",
      "img_red_mean_date2      0\n",
      "img_green_mean_date2    0\n",
      "img_blue_mean_date2     0\n",
      "img_red_std_date2       0\n",
      "img_green_std_date2     0\n",
      "img_blue_std_date2      0\n",
      "img_red_mean_date3      0\n",
      "img_green_mean_date3    0\n",
      "img_blue_mean_date3     0\n",
      "img_red_std_date3       0\n",
      "img_green_std_date3     0\n",
      "img_blue_std_date3      0\n",
      "img_red_mean_date4      0\n",
      "img_green_mean_date4    0\n",
      "img_blue_mean_date4     0\n",
      "img_red_std_date4       0\n",
      "img_green_std_date4     0\n",
      "img_blue_std_date4      0\n",
      "img_red_mean_date5      0\n",
      "img_green_mean_date5    0\n",
      "img_blue_mean_date5     0\n",
      "img_red_std_date5       0\n",
      "img_green_std_date5     0\n",
      "img_blue_std_date5      0\n",
      "date0                   0\n",
      "change_status_date0     0\n",
      "date1                   0\n",
      "change_status_date1     0\n",
      "date2                   0\n",
      "change_status_date2     0\n",
      "date3                   0\n",
      "change_status_date3     0\n",
      "date4                   0\n",
      "change_status_date4     0\n",
      "index                   0\n",
      "geometry                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# fill missing values for columns with numbers or dates with the mean of the column\n",
    "for column in train_df.columns:\n",
    "    if train_df[column].dtype == np.float64 or train_df[column].dtype == np.int64:\n",
    "        train_df[column] = train_df[column].fillna(train_df[column].mean())\n",
    "    if train_df[column].dtype == np.datetime64:\n",
    "        train_df[column] = train_df[column].fillna(train_df[column].mean())\n",
    "    if train_df[column].dtype == np.object_:\n",
    "        train_df[column] = train_df[column].fillna(train_df[column].mode()[0])\n",
    "\n",
    "# count number of missing values in each column\n",
    "missing_values = train_df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordering dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date0       date1       date2       date3       date4\n",
      "0       01-08-2018  09-12-2013  10-09-2016  22-07-2019  24-07-2017\n",
      "1       01-08-2018  09-12-2013  10-09-2016  22-07-2019  24-07-2017\n",
      "2       01-08-2018  09-12-2013  10-09-2016  22-07-2019  24-07-2017\n",
      "3       01-08-2018  09-12-2013  10-09-2016  22-07-2019  24-07-2017\n",
      "4       01-08-2018  09-12-2013  10-09-2016  22-07-2019  24-07-2017\n",
      "...            ...         ...         ...         ...         ...\n",
      "296141  19-11-2014  25-02-2017  27-01-2014  28-03-2018  28-12-2015\n",
      "296142  19-11-2014  25-02-2017  27-01-2014  28-03-2018  28-12-2015\n",
      "296143  19-11-2014  25-02-2017  27-01-2014  28-03-2018  28-12-2015\n",
      "296144  19-11-2014  25-02-2017  27-01-2014  28-03-2018  28-12-2015\n",
      "296145  19-11-2014  25-02-2017  27-01-2014  28-03-2018  28-12-2015\n",
      "\n",
      "[296146 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create the table of train data\n",
    "train_dates = train_df[[f'date{i}' for i in range(0, 5)]]\n",
    "print(train_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Calculating arsort indexes of dates\n",
    "def index_sort_by_dates(date_row):\n",
    "    return np.argsort(pd.to_datetime(date_row, format='%d-%m-%Y'))\n",
    "\n",
    "# Test index_sort_by_dates\n",
    "print(index_sort_by_dates(['18-09-2010', '18-09-2009', '18-09-2000']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date0  date1  date2  date3  date4\n",
      "0           1      2      4      0      3\n",
      "1           1      2      4      0      3\n",
      "2           1      2      4      0      3\n",
      "3           1      2      4      0      3\n",
      "4           1      2      4      0      3\n",
      "...       ...    ...    ...    ...    ...\n",
      "296141      2      0      4      1      3\n",
      "296142      2      0      4      1      3\n",
      "296143      2      0      4      1      3\n",
      "296144      2      0      4      1      3\n",
      "296145      2      0      4      1      3\n",
      "\n",
      "[296146 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create the table of arsort indexes of dates\n",
    "train_index = train_dates[[f'date{i}' for i in range(0, 5)]].apply(index_sort_by_dates, axis=1)\n",
    "print(train_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Times Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[365, 365, 366, 2922]\n"
     ]
    }
   ],
   "source": [
    "def time_interval(row):\n",
    "    date_row = row[:5]\n",
    "    index_row = row[5:]\n",
    "    return [int((pd.to_datetime(date_row[index_row[i + 1]], format='%d-%m-%Y') - pd.to_datetime(date_row[index_row[i]], format='%d-%m-%Y')).days) if date_row[index_row[i]] is not None and date_row[index_row[i + 1]] is not None else None for i in range(4)]\n",
    "\n",
    "# Test time_interval\n",
    "print(time_interval(['18-09-2020', '18-09-2009', '18-09-2010', '18-09-2011', '18-09-2012', 1, 2, 3, 4, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        time_interval1  time_interval2  time_interval3  time_interval4\n",
      "0                 1006             317             373             355\n",
      "1                 1006             317             373             355\n",
      "2                 1006             317             373             355\n",
      "3                 1006             317             373             355\n",
      "4                 1006             317             373             355\n",
      "...                ...             ...             ...             ...\n",
      "296141             296             404             425             396\n",
      "296142             296             404             425             396\n",
      "296143             296             404             425             396\n",
      "296144             296             404             425             396\n",
      "296145             296             404             425             396\n",
      "\n",
      "[296146 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create the table of time intervals\n",
    "train_intervals = pd.concat([train_dates, train_index], axis=1).apply(time_interval, axis=1)\n",
    "train_intervals = pd.DataFrame(train_intervals.tolist(), columns=['time_interval1', 'time_interval2', 'time_interval3', 'time_interval4'])\n",
    "print(train_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_interval1    0\n",
      "time_interval2    0\n",
      "time_interval3    0\n",
      "time_interval4    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count number of None\n",
    "none_counts = train_intervals.isnull().sum()\n",
    "print(none_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296146, 4)\n",
      "[[1006  317  373  355]\n",
      " [1006  317  373  355]\n",
      " [1006  317  373  355]\n",
      " ...\n",
      " [ 296  404  425  396]\n",
      " [ 296  404  425  396]\n",
      " [ 296  404  425  396]]\n"
     ]
    }
   ],
   "source": [
    "# Creating array\n",
    "train_intervals = np.asarray(train_intervals).reshape(-1, 4)\n",
    "print(train_intervals.shape)\n",
    "print(train_intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colors Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 4, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "# Define the get_ordered_values function\n",
    "def get_ordered_values(row):\n",
    "    data = row[:5]\n",
    "    indexes = row[5:]\n",
    "    return [data[int(indexes[i])] for i in range(5)]\n",
    "\n",
    "# Test get_ordered_values\n",
    "print(get_ordered_values([1, 2, 3, 4, 5, 4, 3, 2, 1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red-Green-Blue Means and Svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        img_red_std_date1  img_red_std_date2  img_red_std_date3   \n",
      "0                0.014411           1.975850           0.304269  \\\n",
      "1               -0.300026           1.035275           0.095744   \n",
      "2                0.061137           2.161605           0.345593   \n",
      "3                0.413590           1.651254           0.136546   \n",
      "4               -0.248410           0.339109           0.756329   \n",
      "...                   ...                ...                ...   \n",
      "296141          -0.405247          -0.081253           0.676931   \n",
      "296142           0.561150           2.002278           1.856386   \n",
      "296143          -0.795748          -0.387250          -0.104333   \n",
      "296144           0.113774           0.359181           1.914090   \n",
      "296145           0.123668           0.949116           1.186471   \n",
      "\n",
      "        img_red_std_date4  img_red_std_date5  img_blue_std_date1   \n",
      "0                0.123766           0.883730           -0.027335  \\\n",
      "1               -0.203617           0.699843           -0.369508   \n",
      "2               -0.251339           1.067636           -0.208102   \n",
      "3               -0.072178           0.835331           -0.107265   \n",
      "4                1.632433           0.047026            1.171389   \n",
      "...                   ...                ...                 ...   \n",
      "296141          -0.201141           0.331139           -0.300929   \n",
      "296142           0.213999           1.772325            1.765827   \n",
      "296143          -0.765861          -0.912609           -1.028712   \n",
      "296144           0.117582          -0.254436           -0.225279   \n",
      "296145           0.127192          -0.491259           -0.197161   \n",
      "\n",
      "        img_blue_std_date2  img_blue_std_date3  img_blue_std_date4   \n",
      "0                 1.318373            0.657390            0.007379  \\\n",
      "1                 0.763380            0.754519           -0.227514   \n",
      "2                 1.404730            0.710871           -0.285121   \n",
      "3                 1.159886            0.750186           -0.425119   \n",
      "4                 1.728518            0.830831            1.961199   \n",
      "...                    ...                 ...                 ...   \n",
      "296141            0.544730            0.501337           -0.517804   \n",
      "296142            1.833586            2.296915           -0.301149   \n",
      "296143           -0.481106           -0.061793           -1.019308   \n",
      "296144            0.369317            2.040707           -0.213945   \n",
      "296145            1.087357            1.526112           -0.185759   \n",
      "\n",
      "        img_blue_std_date5  ...  img_green_mean_date1  img_green_mean_date2   \n",
      "0                 0.285181  ...              0.766525              1.219021  \\\n",
      "1                 0.118679  ...              0.926324              2.001288   \n",
      "2                 0.454950  ...              0.530515              1.205735   \n",
      "3                 0.295954  ...              0.420980              1.159946   \n",
      "4                 0.220945  ...              1.665871              1.815816   \n",
      "...                    ...  ...                   ...                   ...   \n",
      "296141           -0.110336  ...              2.503810              3.185845   \n",
      "296142            2.519114  ...              2.023350              0.796151   \n",
      "296143           -0.943882  ...             -1.446407             -0.580109   \n",
      "296144           -0.155539  ...             -0.802531              0.578914   \n",
      "296145           -0.519340  ...             -0.666826              1.509131   \n",
      "\n",
      "        img_green_mean_date3  img_green_mean_date4  img_green_mean_date5   \n",
      "0                  -0.445570             -0.236083             -0.747323  \\\n",
      "1                  -0.376868             -0.242300             -0.371502   \n",
      "2                  -0.541917             -0.068858             -0.675626   \n",
      "3                  -0.748831             -0.433780             -0.738664   \n",
      "4                   0.662266              2.051521             -0.491421   \n",
      "...                      ...                   ...                   ...   \n",
      "296141              1.720884              0.018885              1.124865   \n",
      "296142              1.056727             -0.945687              0.347284   \n",
      "296143             -0.441758             -1.437690             -1.742195   \n",
      "296144              1.057625             -0.831491              1.263733   \n",
      "296145              1.573825             -0.703727              1.391570   \n",
      "\n",
      "        img_green_std_date1  img_green_std_date2  img_green_std_date3   \n",
      "0                  0.122706             1.555535             0.425460  \\\n",
      "1                 -0.342595             0.883909             0.439272   \n",
      "2                 -0.083822             1.593279             0.459618   \n",
      "3                  0.101474             1.332655             0.393506   \n",
      "4                  0.547124             1.075713             0.774511   \n",
      "...                     ...                  ...                  ...   \n",
      "296141            -0.333858             0.417710             0.295694   \n",
      "296142             1.272627             1.314181             1.987976   \n",
      "296143            -0.931836            -0.564291            -0.037696   \n",
      "296144             0.004484             0.310341             1.977281   \n",
      "296145             0.001828             0.955820             1.100167   \n",
      "\n",
      "        img_green_std_date4  img_green_std_date5  \n",
      "0                  0.151680             0.399959  \n",
      "1                 -0.274297             0.203233  \n",
      "2                 -0.302571             0.564793  \n",
      "3                 -0.169267             0.392713  \n",
      "4                  2.341209             0.208099  \n",
      "...                     ...                  ...  \n",
      "296141            -0.337194             0.188302  \n",
      "296142            -0.198184             2.217007  \n",
      "296143            -0.907596            -0.983789  \n",
      "296144             0.028962            -0.176872  \n",
      "296145             0.026305            -0.504235  \n",
      "\n",
      "[296146 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "config = {'red_mean', 'blue_mean', 'green_mean', 'red_std', 'blue_std', 'green_std'}\n",
    "train_colors = pd.DataFrame()\n",
    "\n",
    "for element in config:\n",
    "    train_element = pd.concat([train_df[[f'img_{element}_date{i}' for i in range(1, 6)]], train_index], axis=1).apply(get_ordered_values, axis=1)\n",
    "    train_element = pd.DataFrame(train_element.tolist(), columns=[f'img_{element}_date{i}' for i in range(1, 6)])\n",
    "    train_element = train_element.fillna(train_element.mean())\n",
    "    train_element = (train_element - train_element.mean()) / train_element.std()\n",
    "    train_colors = pd.concat([train_colors, train_element], axis=1)\n",
    "\n",
    "print(train_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296146, 30)\n"
     ]
    }
   ],
   "source": [
    "# Creating array\n",
    "train_colors = np.asarray(train_colors).reshape(-1, 30)\n",
    "print(train_colors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hot-one Encoding for Urban and Geographical data\n",
    "\n",
    "### Change Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         [Greenland, Construction Started, Construction...\n",
      "1         [Greenland, Land Cleared, Construction Midway,...\n",
      "2         [Greenland, Land Cleared, Land Cleared, Constr...\n",
      "3         [Greenland, Construction Started, Construction...\n",
      "4         [Prior Construction, Prior Construction, Prior...\n",
      "                                ...                        \n",
      "296141    [Land Cleared, Greenland, Land Cleared, Constr...\n",
      "296142    [Greenland, Greenland, Land Cleared, Construct...\n",
      "296143    [Greenland, Greenland, Greenland, Construction...\n",
      "296144    [Land Cleared, Land Cleared, Land Cleared, Lan...\n",
      "296145    [Greenland, Greenland, Greenland, Greenland, C...\n",
      "Length: 296146, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Get change_status by ascending dates\n",
    "train_change_status = pd.concat([train_df[[f'change_status_date{i}' for i in range(0, 5)]], train_index], axis=1).apply(get_ordered_values, axis=1)\n",
    "print(train_change_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Filtering\n",
    "\n",
    "### Computing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4c/2cw0jjvs6nb5v4xdxk3ty9s40000gn/T/ipykernel_19243/2481818807.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  train_area = np.asarray(train_df[['geometry']].area)\n"
     ]
    }
   ],
   "source": [
    "# Area of the building\n",
    "train_area = np.asarray(train_df[['geometry']].area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_convex_hull_area(geometry):\n",
    "    try:\n",
    "        convex_hull = geometry.convex_hull\n",
    "        return convex_hull.area\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Calculate the area of the convex hull for each geometry\n",
    "train_convex_area = train_df['geometry'].apply(calculate_convex_hull_area)\n",
    "\n",
    "# Convert to numpy array\n",
    "train_convex_area = np.asarray(train_convex_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convex hull ratio\n",
    "train_convex_ratio = train_area / train_convex_area\n",
    "train_convex_ratio = train_convex_ratio.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the area parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non convex areas: 42369\n",
      "Ratio of non convex areas of 14.306794621571791%.\n",
      "Mean area: 5.792798723701533e-07 and std area: 6.544386914765251e-05\n"
     ]
    }
   ],
   "source": [
    "# Number of non convex areas\n",
    "non_convex_areas_n = np.sum(train_convex_ratio < 0.99)\n",
    "non_convex_areas_ratio = non_convex_areas_n / len(train_convex_ratio)\n",
    "print('Number of non convex areas: ' + str(non_convex_areas_n))\n",
    "print('Ratio of non convex areas of ' + str(non_convex_areas_ratio*100) + '%.')\n",
    "mean_area = np.mean(train_area)\n",
    "\n",
    "# Mean and Standard deviation of areas\n",
    "std_area = np.std(train_area)\n",
    "print('Mean area: ' + str(mean_area) + ' and std area: ' + str(std_area))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coherent ! a road takes less place than a residential area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00363946  0.28623304]\n",
      " [-0.00213689  0.28623304]\n",
      " [ 0.00369309  0.28623304]\n",
      " ...\n",
      " [-0.00868493  0.28623304]\n",
      " [-0.00882395  0.28623304]\n",
      " [-0.00881304  0.28623304]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize area\n",
    "train_area = (train_area - train_area.mean()) / train_area.std()\n",
    "train_convex_ratio = (train_convex_ratio - train_convex_ratio.mean()) / train_convex_ratio.std()\n",
    "\n",
    "# Stacking the features\n",
    "train_area_features = np.hstack([train_area.reshape(-1, 1), train_convex_ratio])\n",
    "print(train_area_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate all the tables and PCA\n",
    "\n",
    "### Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296146, 36)\n"
     ]
    }
   ],
   "source": [
    "# Concatenating the features\n",
    "train_features = np.hstack([train_area_features, train_colors, train_intervals])\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2718283777478106\n"
     ]
    }
   ],
   "source": [
    "# Applying PCA\n",
    "W = np.dot(train_features.T, train_features) # compute covariance matrix\n",
    "eigval, eigvec = np.linalg.eig(W) # compute eigenvalues and eigenvectors of covariance matrix\n",
    "\n",
    "# Sort eigenvalues and eigenvectors\n",
    "idx = eigval.argsort()[::-1] # Sort eigenvalues\n",
    "eigvec = eigvec[:,idx] # Sort eigenvectors according to eigenvalues\n",
    "eigval = eigval[idx] # Sort eigenvalues\n",
    "\n",
    "print(np.sum(eigval[:1]) / np.sum(eigval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the best k to achieve 85% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=12 ; percent=86%.\n"
     ]
    }
   ],
   "source": [
    "k = 1\n",
    "while k < len(eigval) and (np.sum(eigval[:k])/np.sum(eigval)) < 0.85:   \n",
    "    k += 1\n",
    "\n",
    "print('k=' + str(k), '; percent=' + str(int(np.sum(eigval[:k]) / np.sum(eigval)*100)) + '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296146, 12)\n",
      "[[-1.3431028   1.74403024 -0.66057054 ... -0.89569744 -2.68677199\n",
      "   0.29551962]\n",
      " [-1.48372117  0.04724137 -0.71059037 ... -0.23448728 -2.93330396\n",
      "   0.50741579]\n",
      " [-1.19749859  1.68838922 -0.85279169 ... -1.06727309 -2.43797418\n",
      "   0.18657035]\n",
      " ...\n",
      " [ 5.15659743  0.51523952 -0.07278216 ... -0.13675119  0.14544733\n",
      "  -0.10777262]\n",
      " [-1.3586172   0.82527781  0.01601422 ...  2.56692745  0.76149078\n",
      "   0.16726591]\n",
      " [-2.50098859 -0.37138308 -0.28813019 ...  2.23041491  0.77871598\n",
      "   0.16023501]]\n"
     ]
    }
   ],
   "source": [
    "# Selecting the first k principal components\n",
    "train_features = np.dot(train_features, eigvec[:,:k])\n",
    "print(train_features.shape)\n",
    "print(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train features\n",
    "np.save('preprocessed_data.npy', train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         1\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "4         0\n",
      "         ..\n",
      "296141    3\n",
      "296142    2\n",
      "296143    2\n",
      "296144    2\n",
      "296145    2\n",
      "Name: change_type, Length: 296146, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Save the train labels\n",
    "train_y = train_df['change_type'].apply(lambda x: change_type_map[x])\n",
    "print(train_y)\n",
    "\n",
    "np.save('preprocessed_labels.npy', train_df['change_type'].map(change_type_map).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "test.geojson: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32mfiona/ogrext.pyx:136\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfiona/_err.pyx:291\u001b[0m, in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: test.geojson: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Adapt the test data to the PCA model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest.geojson\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/geopandas/io/file.py:297\u001b[0m, in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m         path_or_bytes \u001b[38;5;241m=\u001b[39m filename\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_fiona\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown engine \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/geopandas/io/file.py:338\u001b[0m, in \u001b[0;36m_read_file_fiona\u001b[0;34m(path_or_bytes, from_bytes, bbox, mask, rows, where, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m     reader \u001b[38;5;241m=\u001b[39m fiona\u001b[38;5;241m.\u001b[39mopen\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fiona_env():\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m features:\n\u001b[1;32m    339\u001b[0m         crs \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mcrs_wkt\n\u001b[1;32m    340\u001b[0m         \u001b[38;5;66;03m# attempt to get EPSG code\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/fiona/env.py:457\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    454\u001b[0m     session \u001b[38;5;241m=\u001b[39m DummySession()\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m env_ctor(session\u001b[38;5;241m=\u001b[39msession):\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/fiona/__init__.py:292\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, allow_unsupported_drivers, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m     path \u001b[38;5;241m=\u001b[39m parse_path(fp)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     colxn \u001b[38;5;241m=\u001b[39m \u001b[43mCollection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43menabled_drivers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menabled_drivers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unsupported_drivers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_unsupported_drivers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    303\u001b[0m     colxn \u001b[38;5;241m=\u001b[39m Collection(\n\u001b[1;32m    304\u001b[0m         path,\n\u001b[1;32m    305\u001b[0m         mode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/fiona/collection.py:243\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, include_fields, wkt_version, allow_unsupported_drivers, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m Session()\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m WritingSession()\n",
      "File \u001b[0;32mfiona/ogrext.pyx:588\u001b[0m, in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfiona/ogrext.pyx:143\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: test.geojson: No such file or directory"
     ]
    }
   ],
   "source": [
    "# Adapt the test data to the PCA model\n",
    "test_df = gpd.read_file('test.geojson', index_col=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
